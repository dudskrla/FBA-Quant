{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# [HW4] FBA QUANT - SYSTEMATIC MARKET MAKING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Kim Na Young (dudskrla09@gmail.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 1. Foundations of Reinforcement Learning with Applications in Finance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please run the Python codes in Chapter 5. Dynamic Programming Algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://github.com/TikhonJelvis/RL-book/tree/master/rl/chapter5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 2. Analytic Optimal Actions and Cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider a continuous-states, continuous-actions, discrete-time, non-terminating MDP with state space as $ R $ and action space as $ R $. When in state $ s ∈ R $ , upon taking action $ a ∈ R $, one transitions to next state $ s' ∈ R $ according to a normal distribution $ s^{'} $ ~ $ N(s,σ^2) $ for a fixed variance $ σ^2∈R^+ $. The corresponding cost associated with this transition is $ e^{as'} $, i.e., the cost depends on the action $ a $ and the state $ s' $ one transitions to. The problem is to minimize the infinite-horizon Expected Discounted-Sum of Costs (with discount factor $ γ<1 $). For this assignment, solve this problem just for the special case of $ γ=0 $ (i.e., the myopic case) using elementary calculus. Derive an analytic expression for the optimal action in any state and the corresponding optimal cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The problem is to minimize the infinite-horizon Expected Discounted-Sum of Costs\n",
        "- $\\gamma = 0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(1) \n",
        "$$ V^*(s) = max_{a∈A} \\{{R(s,a) + γ\\sum_{s'∈N}P(s, a, s')*V^*(s')\\}}  $$\n",
        "$$ = max_{a∈A} \\{{R(s,a) \\}} \\; (∵ \\; γ=0) $$\n",
        "\n",
        "\n",
        "(2)\n",
        "$$ R(s, a) = E[s', r=-e^{as'}|s, a] = -\\int_\\mathbb{R} e^{as'}\\dfrac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-(s' - s)^2/2\\sigma^2} ds' $$\n",
        "\n",
        "$$ = -\\int_\\mathbb{R}\\dfrac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\dfrac{s'^2 - 2ss' - 2\\sigma^2 as' + s^2}{2\\sigma^2}} ds' $$\n",
        "\n",
        "$$ = - e^{ \\dfrac{(s + \\sigma^2 a)^2 - s^2}{2\\sigma^2} } \\int_\\mathbb{R}\\dfrac{1}{\\sqrt{2\\pi \\sigma^2}}\n",
        "e^{- \\frac{(s' - (s + \\sigma^2 a))^2}{2\\sigma^2} } ds' $$\n",
        "\n",
        "$$ = - e^{ \\dfrac{(s + \\sigma^2 a)^2 - s^2}{2\\sigma^2} } \\; (∵ \\; \\int_\\mathbb{R}\\dfrac{1}{\\sqrt{2\\pi \\sigma^2}}\n",
        "e^{- \\frac{(s' - (s + \\sigma^2 a))^2}{2\\sigma^2} } ds' = 1) $$\n",
        "\n",
        "the probability density function of the distribution $ N(s+\\sigma^2 a, \\sigma^2) $\n",
        "\n",
        "\n",
        "$$ = - e^{ \\frac{\\sigma^4a^2 + 2s\\sigma^2a}{2\\sigma^2} } $$ \n",
        "\n",
        "$$ = - e^{\\frac{\\sigma^2 a^2}{2} + sa}$$ \n",
        "\n",
        "To find the optimal action that maximize $ V^* $, we differenciate it:\n",
        "\n",
        "$$ (V^*)' = - e^{\\frac{\\sigma^2 a^2}{2} + sa} (s+a\\sigma^2) = 0 $$\n",
        "\n",
        "$$ ∴ \\; Optimal \\; action \\; = a^* = -\\frac{s}{\\sigma^2} $$\n",
        "\n",
        "$$ ∴ \\; Corresponding \\; optimal \\; cost \\; = V^* = e^{-\\frac{s^2}{2\\sigma^2}}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 3. Manual Value Iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider a simple MDP with $ S={s_1,s_2,s_3 },T={s_3 },A={a_1,a_2 } $. The State Transition Probability function\n",
        "\n",
        "$$ P:N×A×S→[0,1] $$\n",
        "\n",
        "is defined as:\n",
        "\n",
        "$$ P(s_1,a_1,s_1 )=0.2,P(s_1,a_1,s_2 )=0.6,P(s_1,a_1,s_3 )=0.2 $$\n",
        "\n",
        "$$ P(s_1,a_2,s_1 )=0.1,P(s_1,a_2,s_2 )=0.2,P(s_1,a_2,s_3 )=0.7 $$ \n",
        "\n",
        "$$ P(s_2,a_1,s_1 )=0.3,P(s_2,a_1,s_2 )=0.3,P(s_2,a_1,s_3 )=0.4 $$\n",
        "\n",
        "$$ P(s_2,a_2,s_1 )=0.5,P(s_2,a_2,s_2 )=0.3,P(s_2,a_2,s_3 )=0.2 $$\n",
        "\n",
        "\n",
        "The Reward Function\n",
        "\n",
        "$$ R:N×A→R $$\n",
        "\n",
        "is defined as:\n",
        "\n",
        "$$ R(s_1,a_1 )=8.0,R(s_1,a_2 )=10.0 $$\n",
        "\n",
        "$$ R(s_2,a_1 )=1.0,R(s_2,a_2 )=-1.0 $$\n",
        "\n",
        "Assume discount factor $ γ=1 $.\n",
        "\n",
        "Your task is to determine an Optimal Deterministic Policy by manually working out (not with code) simply the first two iterations of Value Iteration algorithm.\n",
        "\n",
        "- Initialize the Value Function for each state to be it's max (over actions) reward, i.e., we initialize the Value Function to be $ v_0 (s_1 )=10.0,v_0 (s_2 )=1.0,v_0 (s_3 )=0.0 $. Then manually calculate $ q_k (⋅,⋅) $ and $ v_k (⋅) $ from $ v_{(k-1)}(⋅) $ using the Value Iteration update, and then calculate the greedy policy $ π_k (⋅) $ from $ q_k (⋅,⋅) $ for $ k=1 $ and $ k=2 $ (hence, 2 iterations).\n",
        "\n",
        "- Now argue that $ π_k (⋅) $ for $ k>2 $ will be the same as $ π_2 (⋅) $. Hint: You can make the argument by examining the structure of how you get $ q_k (⋅,⋅) $ from $ v_(k-1) (⋅) $. With this argument, there is no need to go beyond the two iterations you performed above, and so you can establish $ π_2 (⋅) $ as an Optimal Deterministic Policy for this MDP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Value function : $q_k(s,a) =  { R(s,a) + \\gamma \\cdot \\sum_{s'} P(s,a,s') \\cdot V_{k-1}{(s')} }$\n",
        "\n",
        "∵ Discounting rate $\\gamma = 1$, value functions are written:\n",
        "\n",
        "$q_k(s_i,a_1) =  { R(s_i,a_1) + P(s_i,a_1,s_1) \\cdot V_{k-1}(s_1) + P(s_i,a_1,s_2) \\cdot V_{k-1}(s_2) }$\n",
        "\n",
        "$q_k(s_i,a_2) =  { R(s_i,a_2) + P(s_i,a_2,s_1) \\cdot V_{k-1}(s_1) + P(s_i,a_2,s_2) \\cdot V_{k-1}(s_2) }$\n",
        "\n",
        "$V_{k}(s_i) = \\max \\left\\{ q_k(s_i,a_1), q_k(s_i, a_2)\\right\\}$\n",
        "\n",
        "1) Itertation 1 : $q_1(s, a)$ using initial values\n",
        "\n",
        "   1) $s_1$:\n",
        "   $$q_1(s_1, a_1) = 8.0 + 0.2 \\cdot 10.0 + 0.6 \\cdot 1.0 = 10.6$$\n",
        "   $$q_1(s_1, a_2) = 10.0 + 0.1 \\cdot 10.0 + 0.2 \\cdot 1.0 = 11.2$$\n",
        "   $$v_1(s_1) = \\max \\{ q_1(s_1, a_1), q_1(s_1, a_2) \\} = 11.2$$\n",
        "   \n",
        "   $ q_1(s_1, a_2) $ is bigger than others. \n",
        "   ∴ $\\pi_1(s_1) = a_2$.\n",
        "\n",
        "   2) $s_2$:\n",
        "   $$q_1(s_2, a_1) = 1.0 + 0.3 \\cdot 10.0 + 0.3 \\cdot 1.0 = 4.3$$\n",
        "   $$q_1(s_2, a_2) = -1.0 + 0.5 \\cdot 10.0 + 0.3 \\cdot 1.0 = 4.3$$\n",
        "   $$v_1(s_2) = \\max \\{ q_1(s_2, a_1), q_1(s_2, a_2) \\} = 4.3$$\n",
        "   \n",
        "   ∴ $\\pi_1(s_2) = a_1  \\; or \\; a_2$\n",
        "\n",
        "2) Iteration 2 : $q_2(s, a)$ using existing value functions\n",
        "\n",
        "   1) $s_1$:\n",
        "   $$q_2(s_1, a_1) = 8.0 + 0.2 \\cdot 11.2 + 0.6 \\cdot 4.3 = 12.82$$\n",
        "   $$q_2(s_1, a_2) = 10.0 + 0.1 \\cdot 11.2 + 0.2 \\cdot 4.3 = 11.98$$\n",
        "   $$v_2(s_1) = \\max \\{ q_2(s_1, a_1), q_2(s_1, a_2) \\} = 12.82$$\n",
        "   \n",
        "   $ q_2(s_1, a_1) $ is bigger than others.\n",
        "   ∴ $\\pi_2(s_1) = a_1$\n",
        "\n",
        "   2) $s_2$:\n",
        "   $$q_2(s_2, a_1) = 1.0 + 0.3 \\cdot 11.2 + 0.3 \\cdot 4.3 = 5.65$$\n",
        "   $$q_2(s_2, a_2) = -1.0 + 0.5 \\cdot 11.2 + 0.3 \\cdot 4.3 = 5.89$$\n",
        "   $$v_2(s_2) = \\max \\{ q_2(s_2, a_1), q_2(s_2, a_2) \\} = 5.89$$\n",
        "   \n",
        "   $ q_2(s_2, a_2) $ is bigger than others.\n",
        "   ∴ $\\pi_2(s_2) = a_2$\n",
        "\n",
        "\n",
        "3) Argue that $ π_k (⋅) $ for $ k>2 $ will be the same as $ π_2 (⋅) $. \n",
        "\n",
        "Hint: You can make the argument by examining the structure of how you get $ q_k (⋅,⋅) $ from $ v_(k-1) (⋅) $ \n",
        "\n",
        "\n",
        "To determine $π_k (⋅) $ for $ k>2 $, we compare:\n",
        "   - for $s_1$, $q_k(s_1, a_1)$ and $q_k(s_1, a_2)$\n",
        "   - for $s_2$, $q_k(s_2, a_1)$ and $q_k(s_2, a_2)$\n",
        "   \n",
        "   1)  for $s_1$: \n",
        "   $$q_k(s_1, a_1) - q_k(s_1, a_2) = (R(s_1, a_1) - R(s_1, a_2)) + V_{k-1}(P(s_1, a_1, s_1) - P(s_1, a_2, s_1)) + V_{k-1}(P(s_1, a_1, s_2) - P(s_1, a_2, s_2))$$\n",
        "   $$ = -2.0 + 0.1 \\cdot v_{k-1}(s_1) + 0.4 \\cdot v_{k-1}(s_2)$$\n",
        "\n",
        "   2) for $s_2$: \n",
        "   $$q_k(s_2, a_2) - q_k(s_2, a_1) = (R(s_2, a_2) - R(s_2, a_1)) + V_{k-1}(P(s_2, a_2, s_1) - P(s_2, a_1, s_1)) + V_{k-1}(P(s_2, a_2, s_2) - P(s_2, a_1, s_2)) $$\n",
        "   $$ = -2.0 + 0.2 \\cdot v_{k-1}(s_1)$$\n",
        "\n",
        "About $V_{k-1}(s_1)$ and $V_{k-1}(s_2) $, \n",
        "\n",
        "(Assumption):\n",
        "   $q_k(s,a) =  { R(s,a) + \\gamma \\cdot \\sum_{s'} P(s,a,s') \\cdot V_{k-1}{(s')} }$\n",
        "\n",
        "   (1) $ R(s, a) $ is positive\n",
        "\n",
        "   (2) $ P(s,a,s') $ is non-negative\n",
        "\n",
        "   (3) $ V_{k-1}{(s')} $ is non-negative \n",
        "\n",
        "   ∴ $ q_k(s,a) $ is positive\n",
        "   \n",
        "   ∴ $ V_k(s_1) $, $ V_k(s_2) $ is positive\n",
        "\n",
        "\n",
        "Given $V_{k-1}(s_1) \\geq 12.82$ and $V_{k-1}(s_2) \\geq 5.89$ for all $k \\geq 3$, it follows that:\n",
        "\n",
        "- $q_k(s_1, a_1) - q_k(s_1, a_2) \\geq -2 + 0.1 \\cdot 12.82 + 0.4 \\cdot 5.89 = 1.638 >0$ for all $k \\geq 3$\n",
        "- $q_k(s_2, a_2) - q_k(s_2, a_1) \\geq -2 + 12.89*0.2 = 0.578 > 0$ for all $k \\geq 3$\n",
        "\n",
        "∴ $ π_k (⋅) $ for $ k>2 $ will be the same as $ π_2 (⋅) $. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 4. Job-Hopping and Wages-Utility-Maximization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You are a worker who starts every day either employed or unemployed. If you start your day employed, you work on your job for the day (one of n jobs, as elaborated later) and you get to earn the wage of the job for the day. However, at the end of the day, you could lose your job with probability $ α∈[0,1] $, in which case you start the next day unemployed. If at the end of the day, you do not lose your job (with probability 1-α ), then you will start the next day with the same job (and hence, the same daily wage). On the other hand, if you start your day unemployed, then you will be randomly offered one of n jobs with daily wages $ w_1,w_2,…w_n∈R^+ $ with respective job-offer probabilities $ p_1,p_2,…p_n∈[0,1] $ (with $ ∑_(i=1)^n p_i=1 $ ). You can choose to either accept or decline the offered job. If you accept the job-offer, your day progresses exactly like the employed-day described above (earning the day's job wage and possibly (with probability α ) losing the job at the end of the day). However, if you decline the job-offer, you spend the day unemployed, receive the unemployment wage w_0∈R^+for the day, and start the next day unemployed. The problem is to identify the optimal choice of accepting or rejecting any of the job-offers the worker receives, in a manner that maximizes the infinite-horizon Expected Discounted-Sum of Wages Utility. Assume the daily discount factor for wages (employed or unemployed) is $ γ∈[0,1) $ . Assume Wages Utility function to be $ U(w)=log⁡(w) $ for any wage amount $ w∈R^+ $. So you are looking to maximize\n",
        "\n",
        "$$\\mathbb{E}[\\sum_{u=t}^\\infty \\gamma^{u-t} \\cdot \\log(w_{i_u})]$$\n",
        "\n",
        "at the start of a given day $ t ( w_(i_u ) $ is the wage earned on day $ u,0≤i_u≤n $ for all $ u≥t $ ).\n",
        "\n",
        "- Express with clear mathematical notation the state space, action space, transition function, reward function, and write the Bellman Optimality Equation customized for this MDP.\n",
        "\n",
        "- You can solve this Bellman Optimality Equation (hence, solve for the Optimal Value Function and the Optimal Policy) with a numerical iterative algorithm (essentially a Dynamic Programming algorithm customized to this problem). Write Python code for this numerical algorithm. Clearly define the inputs and outputs of your algorithm with their types (int, float, List, Mapping etc.). For this problem, don't use any of the MDP/DP code from the git repo, write this customized algorithm from scratch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Mathematical notation\n",
        "    1. state space: $ S = \\{E, U\\} $ : E: employed / U: unemployed \n",
        "\n",
        "    2. action space\n",
        "        - 1) $ S = E$: $ A={Continue} $\n",
        "        - 2) $ S = U$ : $A={Accept, Decline}$ \n",
        "\n",
        "    3. transition function ($T(s, a, s')$)\n",
        "        - 1) from $S=E $ to $ S'=E$ : $1-\\alpha$\n",
        "        - 2) from $S=E $ to $ S'=U$ : $\\alpha$\n",
        "        - 3) from $S=U $ to $ S'=E$ : respective job-offer probability (ex. $p_1$) (if $Accept$)\n",
        "        - 4) from $S=U $ to $ S'=U$ : $1$ (if $Decline$)\n",
        "\n",
        "    4. reward function ($R(s, a)$)\n",
        "        - 1) action: $Accept$ : wage of job\n",
        "        - 2) action: $Decline$ : unemployment wage\n",
        "\n",
        "    5. Bellman Optimality Equation\n",
        "\n",
        "    $$ B*(V)(s) = max_{a∈A} \\{ \\sum_{s'∈S} \\sum_{r∈D} P_R(s, a, r, s') * (r+\\gamma*W(s'))   \\} \\; for \\; all \\; s∈N $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem 5. Two-Stores Inventory Control"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We extend the capacity-constrained inventory example implemented in\n",
        "[rl/chapter3/simple_inventory_mdp_cap.py](https://github.com/TikhonJelvis/RL-book/blob/master/rl/chapter3/simple_inventory_mdp_cap.py) as a FiniteMarkovDecisionProcess (the Finite MDP model for the capacity-constrained inventory example is described in detail in Chapters 1 and 2 of the RLForFinanceBook). Here we assume that we have two different stores, each with their own separate capacities $ C_1 $ and $ C_2 $, their own separate Poisson probability distributions of demand (with means $ λ_1 $ and $ λ_2 $ ), their own separate holding costs $ h_1 $ and $ h_2 $ , and their own separate stockout costs $ p_1 $  and $ p_2 $. At 6 pm upon stores closing each evening, each store can choose to order inventory from a common supplier (as usual, ordered inventory will arrive at the store 36 hours later). We are also allowed to transfer inventory from one store to another, and any such transfer happens overnight, i.e., will arrive by 6 am next morning (since the stores are fairly close to each other). Note that the orders are constrained such that following the orders on each evening, each store's inventory position (sum of on-hand inventory and on-order inventory) cannot exceed the store's capacity (this means the action space is constrained to be finite). Each order made to the supplier incurs a fixed transportation cost of $ K_1 $ (fixed-cost means the cost is the same no matter how many units of non-zero inventory a particular store orders). Moving any non-zero inventory between the two stores incurs a fixed transportation cost of $ K_2 $.\n",
        "\n",
        "Model this as a derived class of FiniteMarkovDecisionProcess much like we did for SimpleInventoryMDPCap in the code repo. Set up instances of this derived class for different choices of the problem parameters (capacities, costs etc.), and determine the Optimal Value Function and Optimal Policy by invoking the function value_iteration (or policy_iteration) from file rl/dynamic_programming.py.\n",
        "\n",
        "Analyze the obtained Optimal Policy and verify that it makes intuitive sense as a function of the problem parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fba",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
